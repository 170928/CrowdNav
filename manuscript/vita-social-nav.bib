@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
	CTLuse_url = "no",
	CTLuse_article_number = "yes",
	CTLuse_paper = "yes",
	CTLuse_forced_etal = "yes",
	CTLmax_names_forced_etal = "4",
	CTLnames_show_etal = "1",
	CTLuse_alt_spacing = "yes",
	CTLalt_stretch_factor = "4",
	CTLdash_repeated_names = "yes",
	CTLname_latex_cmd = ""
}
	
@article{bruin_integrating_2018,
	title = {Integrating {State} {Representation} {Learning} {Into} {Deep} {Reinforcement} {Learning}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2800101},
	abstract = {Most deep reinforcement learning techniques are unsuitable for robotics, as they require too much interaction time to learn useful, general control policies. This problem can be largely attributed to the fact that a state representation needs to be learned as a part of learning control policies, which can only be done through fitting expected returns based on observed rewards. While the reward function provides information on the desirability of the state of the world, it does not necessarily provide information on how to distill a good, general representation of that state from the sensory observations. State representation learning objectives can be used to help learn such a representation. While many of these objectives have been proposed, they are typically not directly combined with reinforcement learning algorithms. We investigate several methods for integrating state representation learning into reinforcement learning. In these methods, the state representation learning objectives help regularize the state representation during the reinforcement learning, and the reinforcement learning itself is viewed as a crucial state representation learning objective and allowed to help shape the representation. Using autonomous racing tests in the TORCS simulator, we show how the integrated methods quickly learn policies that generalize to new environments much better than deep reinforcement learning without state representation learning.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Bruin, T. de and Kober, J. and Tuyls, K. and Babuška, R.},
	month = jul,
	year = {2018},
	keywords = {autonomous racing tests, Deep learning in robotics and automation, deep reinforcement learning techniques, desirability, learning (artificial intelligence), Learning (artificial intelligence), learning and adaptive systems, learning control policies, Machine learning, Robot sensing systems, robotics, sensor fusion, sensory observations, Shape, state representation learning integration, Task analysis, TORCS simulator, Training},
	pages = {1394--1401},
	file = {Bruin 等。 - 2018 - Integrating State Representation Learning Into Dee.pdf:C\:\\Users\\yueji\\Zotero\\storage\\EQH7F3N7\\Bruin 等。 - 2018 - Integrating State Representation Learning Into Dee.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\yueji\\Zotero\\storage\\BEUGJPAB\\8276247.html:text/html}
}

@article{lesort_state_2018,
	title = {State {Representation} {Learning} for {Control}: {An} {Overview}},
	shorttitle = {State {Representation} {Learning} for {Control}},
	url = {http://arxiv.org/abs/1802.04181},
	abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
	urldate = {2018-08-25},
	journal = {arXiv:1802.04181 [cs, stat]},
	author = {Lesort, Timothée and Díaz-Rodríguez, Natalia and Goudou, Jean-François and Filliat, David},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.04181},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1802.04181 PDF:C\:\\Users\\yueji\\Zotero\\storage\\DH8P229J\\Lesort 等。 - 2018 - State Representation Learning for Control An Over.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yueji\\Zotero\\storage\\E3HYKRP3\\1802.html:text/html}
}